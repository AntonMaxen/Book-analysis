{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config.py\n",
    "\n",
    "Contains the the list of common stop words which will be filtered out during the preprocess and a list of the URL adresses to the books scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = stopwords.words('english') + ['and']\n",
    "URL_LIST = ['https://www.gutenberg.org/files/25830/25830-h/25830-h.htm',\n",
    "            'https://www.gutenberg.org/files/84/84-h/84-h.htm',\n",
    "            'https://www.gutenberg.org/files/32069/32069-h/32069-h.htm',\n",
    "            'https://www.gutenberg.org/files/19362/19362-h/19362-h.htm',\n",
    "            'https://www.gutenberg.org/files/64783/64783-h/64783-h.htm',\n",
    "            'https://www.gutenberg.org/files/64791/64791-h/64791-h.htm',\n",
    "            'https://www.gutenberg.org/files/64790/64790-h/64790-h.htm',\n",
    "            'https://www.gutenberg.org/files/2610/2610-h/2610-h.htm',\n",
    "            'https://www.gutenberg.org/files/32300/32300-h/32300-h.htm',\n",
    "            'https://www.gutenberg.org/files/83/83-h/83-h.htm',\n",
    "            'https://www.gutenberg.org/files/103/103-h/103-h.htm',\n",
    "            'https://www.gutenberg.org/files/64810/64810-h/64810-h.htm',\n",
    "            'https://www.gutenberg.org/files/11659/11659-h/11659-h.htm'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_management.py\n",
    "\n",
    "Handles finding and defining the paths used when saving and using the books scraped from gutenberg.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def get_project_root():\n",
    "    return os.getcwd()\n",
    "\n",
    "\n",
    "def get_bookname_from_url(url):\n",
    "    page = url.split('/')[-1]\n",
    "    name = page.split('.')[0]\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_bookfolder():\n",
    "    root_folder = get_project_root()\n",
    "    book_folder = os.path.join(root_folder, 'Files', 'Books')\n",
    "    return book_folder\n",
    "\n",
    "\n",
    "def get_bookpath(url):\n",
    "    name = get_bookname_from_url(url)\n",
    "    book_folder = get_bookfolder()\n",
    "    bookpath = os.path.join(book_folder, f'{name}.pickle')\n",
    "    return bookpath\n",
    "\n",
    "\n",
    "def book_downloaded(url):\n",
    "    bookpath = get_bookpath(url)\n",
    "    return os.path.isfile(bookpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrape.py\n",
    "\n",
    "Contains the functions which use the URL addresses previously defined to scrape the books from gutenberg.org and save the text body as a pickle file and the title as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "def pickle_result(result):\n",
    "    book_folder = get_bookfolder()\n",
    "    bookpath = get_bookpath(result.url)\n",
    "    Path(book_folder).mkdir(parents=True, exist_ok=True)\n",
    "    with open(bookpath, 'wb+') as p_file:\n",
    "        pickle.dump(result, p_file)\n",
    "\n",
    "    return bookpath\n",
    "\n",
    "\n",
    "def get_page(url):\n",
    "    if book_downloaded(url):\n",
    "        fullpath = get_bookpath(url)\n",
    "    else:\n",
    "        result = requests.get(url)\n",
    "        if result.status_code == 200:\n",
    "            fullpath = pickle_result(result)\n",
    "        else:\n",
    "            raise Exception(f'Request error code: {result.status_code}')\n",
    "\n",
    "    return fullpath\n",
    "\n",
    "\n",
    "def load_page(full_path):\n",
    "    with open(full_path, 'rb') as p_file:\n",
    "        unpickled_page = pickle.load(p_file)\n",
    "\n",
    "    return BeautifulSoup(unpickled_page.text, 'html.parser')\n",
    "\n",
    "\n",
    "def trim_page(soup, identifiers):\n",
    "    for element in soup.find_all(*identifiers):\n",
    "        element.decompose()\n",
    "\n",
    "\n",
    "def filter_page(soup):\n",
    "    removal = True\n",
    "    for i, element in enumerate(soup.body.find_all()):\n",
    "        if '*** START' in element.text:\n",
    "            element.decompose()\n",
    "            removal = False\n",
    "        elif '*** END' in element.text:\n",
    "            removal = True\n",
    "\n",
    "        if removal:\n",
    "            element.decompose()\n",
    "\n",
    "\n",
    "def get_title(soup):\n",
    "    paragraphs = soup.body.find_all()\n",
    "    for p in paragraphs:\n",
    "        if '*** START' in p.text:\n",
    "            title = p.text\n",
    "            title = title.strip()\n",
    "            title = title.encode('ascii', 'ignore').decode()\n",
    "            title = re.sub('\\r', '', title)\n",
    "            title = re.sub('\\n', ' ', title)\n",
    "\n",
    "            regex = re.compile('(\\*{3}[^\\*]*\\*{3})')\n",
    "            match_obj = regex.search(title)\n",
    "            if match_obj:\n",
    "                title = match_obj[0]\n",
    "\n",
    "            title = re.sub('\\*{3} START OF (THIS|THE) PROJECT GUTENBERG EBOOK ', '', title)\n",
    "            title = title.replace('***', '').strip().title()\n",
    "            return title\n",
    "\n",
    "\n",
    "def scrape_url(url):\n",
    "    full_path = get_page(url)\n",
    "    soup = load_page(full_path)\n",
    "    title = get_title(soup)\n",
    "    filter_page(soup)\n",
    "    text_list = soup.body.get_text(separator=\"\\n\", strip=True).split()\n",
    "    book_dict = {'title': title,\n",
    "                 'text_list': text_list}\n",
    "    return book_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess.py\n",
    "\n",
    "Contains all methods used to format and clean the text to and transform it from a binary text to a list containing only terms useful to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n",
    "def stemmer(text_list):\n",
    "    new_text_list = []\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for text in text_list:\n",
    "        stemmed_text = ps.stem(text)\n",
    "        new_text_list.append(stemmed_text)\n",
    "        if text != stemmed_text:\n",
    "            #print(text + \"->\" + stemmed_text)\n",
    "            pass\n",
    "    return new_text_list\n",
    "\n",
    "\n",
    "def lemmatize(text_list):\n",
    "    new_text_list = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for text in text_list:\n",
    "        lemmed_text = lemmatizer.lemmatize(text)\n",
    "        new_text_list.append(lemmed_text)\n",
    "        if text != lemmed_text:\n",
    "            # print(text + \"->\" + lemmed_text)\n",
    "            pass\n",
    "\n",
    "    return new_text_list\n",
    "\n",
    "\n",
    "def remove_encoding(text_list):\n",
    "    text_list = [text.encode('ascii', 'ignore').decode() for text in text_list]\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def remove_symbols(text_list):  # TODO: maybe exculde apostrophes and make separate method?\n",
    "    text_list = [re.sub('[^a-zA-Z0-9 ]', '', text).strip() for text in text_list]\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def remove_singel_characters(text_list):\n",
    "    return [text for text in text_list if len(text) > 1]\n",
    "\n",
    "\n",
    "def remove_empty_strings(text_list):\n",
    "    return [text for text in text_list if text]\n",
    "\n",
    "\n",
    "def text_list_to_lower(text_list):\n",
    "    return [text.lower() for text in text_list]\n",
    "\n",
    "\n",
    "def remove_stop_words(text_list):\n",
    "    return [term for term in text_list if term not in STOPWORDS]\n",
    "\n",
    "\n",
    "def clean_text(text_list):\n",
    "    text_list = remove_encoding(text_list)\n",
    "    text_list = text_list_to_lower(text_list)\n",
    "    text_list = remove_stop_words(text_list)\n",
    "    text_list = remove_symbols(text_list)\n",
    "    text_list = remove_singel_characters(text_list)\n",
    "    text_list = lemmatize(text_list)\n",
    "    text_list = stemmer(text_list)\n",
    "    text_list = remove_singel_characters(text_list)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "book.py\n",
    "\n",
    "Contains the Book and BookHandler classes which are used to hold all information and statistics related to the books. Within the classes are the methods used to calculate tf-idf score and compare books to each other with both matching score and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "class Book:\n",
    "    def __init__(self, title, text_list):\n",
    "        self.title = title\n",
    "        self.text = text_list\n",
    "        self.clean()\n",
    "        self.word_dict = self._word_count()\n",
    "        self.tfidf = None\n",
    "        self.vector = None\n",
    "\n",
    "    def clean(self):\n",
    "        self.text = clean_text(self.text)\n",
    "\n",
    "    def calc_tf(self, word):\n",
    "        return self.word_dict[word] / len(self.text)\n",
    "\n",
    "    def vectorize(self, vocab):\n",
    "        vector = []\n",
    "        for word in vocab:\n",
    "            if word in self.tfidf:\n",
    "                value = self.tfidf[word]\n",
    "            else:\n",
    "                value = 0\n",
    "\n",
    "            vector.append(value)\n",
    "\n",
    "        self.vector = vector\n",
    "\n",
    "    def cosinus_similarity(self, other_book):\n",
    "        dot_product = sum([self.vector[i] * other_book.vector[i] for i, _ in enumerate(self.vector)])\n",
    "        abs_first = sum([num**2 for num in self.vector])**0.5\n",
    "        abs_second = sum([num**2 for num in other_book.vector])**0.5\n",
    "        cos_angle = dot_product / (abs_first * abs_second)\n",
    "        cos_angle = 1 if cos_angle > 1 else cos_angle\n",
    "        return math.acos(cos_angle)\n",
    "\n",
    "    def _word_count(self):\n",
    "        term_dict = {}\n",
    "        for term in self.text:\n",
    "            if term in term_dict:\n",
    "                term_dict[term] += 1\n",
    "            else:\n",
    "                term_dict[term] = 1\n",
    "\n",
    "        return term_dict\n",
    "\n",
    "\n",
    "class BookHandler:\n",
    "    def __init__(self):\n",
    "        self.books = []\n",
    "\n",
    "    def add(self, book):\n",
    "        self.books.append(book)\n",
    "\n",
    "    def remove(self, book):\n",
    "        self.books.remove(book)\n",
    "\n",
    "    def calc_tfidf(self):\n",
    "        for book in self.books:\n",
    "            tf_idf_dict = {}\n",
    "            for term in book.word_dict:\n",
    "                tf_score = book.calc_tf(term)\n",
    "                idf_score = self.calc_idf(term)\n",
    "                tf_idf = tf_score * idf_score\n",
    "                tf_idf_dict[term] = tf_idf\n",
    "\n",
    "            book.tfidf = tf_idf_dict\n",
    "\n",
    "    def calc_idf(self, word):\n",
    "        total_documents = len(self.books)\n",
    "        docs_with_term = len([book for book in self.books if word in book.word_dict])\n",
    "        return math.log((total_documents) / (docs_with_term))\n",
    "\n",
    "    def matching_score(self, query_book):\n",
    "        m_scores = [{'book': b, 'score': 0} for b in self.books]\n",
    "\n",
    "        for word in query_book.word_dict:\n",
    "            for i, book in enumerate(self.books):\n",
    "                if word in book.word_dict:\n",
    "                    m_scores[i]['score'] += book.tfidf[word]\n",
    "\n",
    "        m_scores = sorted(m_scores, key=lambda x: x['score'], reverse=True)\n",
    "        return m_scores\n",
    "\n",
    "    def get_total_vocab(self):\n",
    "        total_vocab = []\n",
    "        for book in self.books:\n",
    "            total_vocab += list(book.word_dict.keys())\n",
    "\n",
    "        return total_vocab\n",
    "\n",
    "    def recommend_book(self, query_book):\n",
    "        self.calc_tfidf()\n",
    "        m_scores = self.matching_score(query_book)\n",
    "        self.add(query_book)\n",
    "        self.calc_tfidf()\n",
    "        vocab = self.get_total_vocab()\n",
    "        query_book.vectorize(vocab)\n",
    "        self.remove(query_book)\n",
    "\n",
    "        angles = []\n",
    "\n",
    "        for book in self.books:\n",
    "            book.vectorize(vocab)\n",
    "            angle = book.cosinus_similarity(query_book)\n",
    "            angles.append({'angle': angle, 'book': book})\n",
    "\n",
    "        sorted_angles = sorted(angles, key=lambda x: x['angle'])\n",
    "        return {\n",
    "            'matching_score': m_scores,\n",
    "            'cosinus_similarity': sorted_angles\n",
    "        }\n",
    "\n",
    "\n",
    "def main():\n",
    "    book_handler = BookHandler()\n",
    "    # query_book_index = 6\n",
    "    # query_book_url = URL_LIST[query_book_index]\n",
    "    query_book_url = random.choice(URL_LIST)\n",
    "    URL_LIST.remove(query_book_url)\n",
    "    for url in URL_LIST:\n",
    "        book_dict = scrape_url(url)\n",
    "        book = Book(book_dict['title'], book_dict['text_list'])\n",
    "        book_handler.add(book)\n",
    "\n",
    "    query_book_dict = scrape_url(query_book_url)\n",
    "    query_book = Book(query_book_dict['title'], query_book_dict['text_list'])\n",
    "    result = book_handler.recommend_book(query_book)\n",
    "    print('Querybook')\n",
    "    print(f'Title: {query_book.title}')\n",
    "    print('-' * 30)\n",
    "    print('Compared books')\n",
    "    print('\\n'.join([f'Title: {book.title}' for book in book_handler.books]))\n",
    "    print('-' * 30)\n",
    "    print(f'The recommended books for ({query_book.title}) are:')\n",
    "    matching_score = result['matching_score']\n",
    "    cosinus_similarity = result['cosinus_similarity']\n",
    "\n",
    "    print('By Matching Score:')\n",
    "    for i, score in enumerate(matching_score):\n",
    "        book = score['book']\n",
    "        score = score['score']\n",
    "        print(f'{i+1}: {book.title}, ({score})')\n",
    "\n",
    "    print('-' * 30)\n",
    "    print('By Cosine Similarity:')\n",
    "    for i, angle in enumerate(cosinus_similarity):\n",
    "        book = angle['book']\n",
    "        angle = angle['angle']\n",
    "        print(f'{i+1}: {book.title}, ({math.degrees(angle)})')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
